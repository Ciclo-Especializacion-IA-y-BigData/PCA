{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "3JlzJTyepCvW"
   },
   "source": [
    "\n",
    "# Reducción de dimensionalidad. PCA (Principal Component Analysis)\n",
    "\n",
    "#### UT4. Aprendizaje no supervisado\n",
    "\n",
    "#### MP. Sistemas de Aprendizaje Automático\n",
    "\n",
    "#### Autores: Cristina Gómez Alonso, Carlos Tessier\n",
    "\n",
    "---\n",
    "\n",
    "Este contenido  está directamente relacionado con el:\n",
    "\n",
    "RA4 – Aplica técnicas de aprendizaje no supervisado relacionándolas con los tipos de problemas que tratan de resolver.\n",
    "\n",
    "En concreto, permite trabajar los siguientes criterios de evaluación:\n",
    "\n",
    "* RA4.a) Caracterización de los problemas que resuelve el aprendizaje no supervisado, como la alta dimensionalidad.\n",
    "* RA4.b) Explicación de técnicas no supervisadas como PCA.\n",
    "* RA4.c) Aplicación práctica del algoritmo utilizando herramientas como Scikit-learn.\n",
    "* RA4.d) Justificación del número de componentes seleccionados en función de la varianza explicada.\n",
    "\n",
    "Además, contribuye al:\n",
    "\n",
    "RA6 – Valoración de la calidad de los resultados obtenidos en la práctica.\n",
    "\n",
    "---\n",
    "\n",
    "## Resultados esperados en el alumnado\n",
    "\n",
    "Al finalizar este bloque, el alumnado debe ser capaz de:\n",
    "\n",
    "* Explicar qué problema resuelve PCA.\n",
    "* Aplicarlo en un entorno práctico (por ejemplo, con Python y Scikit-learn).\n",
    "* Interpretar el significado de la varianza explicada.\n",
    "* Justificar el número de componentes seleccionados.\n",
    "* Relacionar PCA con otras técnicas no supervisadas como el clustering.\n",
    "\n",
    "---\n",
    "\n",
    "## ¿Por qué necesitamos reducir dimensiones?\n",
    "\n",
    "En muchos problemas reales trabajamos con un número elevado de variables o características. Por ejemplo, un dataset de clientes puede incluir edad, ingresos, gasto medio, frecuencia de compra y otras muchas variables. Una imagen puede estar formada por miles de píxeles y un texto puede convertirse en cientos de valores numéricos mediante técnicas de representación como los embeddings.\n",
    "\n",
    "Cuando aumenta el número de variables:\n",
    "\n",
    "* El entrenamiento de los modelos se vuelve más lento.\n",
    "* Resulta más difícil encontrar patrones claros.\n",
    "* Aumenta el riesgo de sobreajuste.\n",
    "* La visualización de los datos se complica.\n",
    "\n",
    "A este fenómeno se le denomina maldición de la dimensionalidad. A medida que crecen las dimensiones, los datos se dispersan más en el espacio y las distancias pierden significado, lo que puede afectar negativamente a algoritmos como el clustering.\n",
    "\n",
    "---\n",
    "\n",
    "## ¿Qué es la reducción de dimensionalidad?\n",
    "\n",
    "La reducción de dimensionalidad consiste en disminuir el número de variables manteniendo la mayor cantidad posible de información relevante.\n",
    "\n",
    "Puede compararse con resumir un libro extenso conservando las ideas principales. Se pierde parte del detalle, pero se mantiene la estructura esencial.\n",
    "\n",
    "Es importante destacar que PCA no elimina variables originales, sino que crea nuevas variables que son combinaciones lineales de las anteriores.\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:50%\" src=\"img/img_1.png\" />\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:50%\" src=\"img/img.png\" />\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## ¿Para qué se utiliza PCA en la práctica?\n",
    "\n",
    "PCA se utiliza habitualmente para:\n",
    "\n",
    "* Acelerar el entrenamiento de modelos.\n",
    "* Mejorar el rendimiento de técnicas de clustering.\n",
    "* Visualizar datos en dos o tres dimensiones.\n",
    "* Reducir ruido en los datos.\n",
    "\n",
    "En el contexto actual, PCA sigue siendo una técnica fundamental en aprendizaje automático clásico. En problemas más complejos, como visión artificial o procesamiento del lenguaje natural, se emplean también técnicas más avanzadas como t-SNE o UMAP para visualización, pero PCA continúa siendo una herramienta base en la formación técnica.\n",
    "\n",
    "---\n",
    "\n",
    "## Idea intuitiva de funcionamiento\n",
    "\n",
    "Si trabajamos con dos variables, por ejemplo altura y peso, los datos pueden estar correlacionados. PCA busca la dirección en la que los datos presentan mayor variación y crea una nueva variable que resume esa información.\n",
    "\n",
    "Estas nuevas variables se denominan componentes principales y se ordenan según la cantidad de varianza que explican:\n",
    "\n",
    "* Componente principal 1: explica la mayor parte de la variabilidad.\n",
    "* Componente principal 2: explica la siguiente mayor parte.\n",
    "\n",
    "De este modo, podemos reducir el número de dimensiones conservando la mayor parte de la información relevante.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "6oTzXbSCpCvy"
   },
   "source": [
    "\n",
    "# 1. La maldición de la dimensionalidad\n",
    "\n",
    "Estamos acostumbrados a pensar en 2 o 3 dimensiones: largo, ancho y alto. Sin embargo, en Machine Learning trabajamos con espacios que pueden tener decenas, cientos o miles de dimensiones. Cada variable del dataset añade una dimensión nueva.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "* 5 variables → espacio de 5 dimensiones\n",
    "* 100 variables → espacio de 100 dimensiones\n",
    "* 1000 variables → espacio de 1000 dimensiones\n",
    "\n",
    "Aunque no podamos visualizar esos espacios, los algoritmos sí trabajan en ellos.\n",
    "\n",
    "---\n",
    "\n",
    "## ¿Qué ocurre cuando aumentan las dimensiones?\n",
    "\n",
    "En espacios de alta dimensión suceden fenómenos poco intuitivos.\n",
    "\n",
    "En dimensiones bajas (2D o 3D), los puntos suelen estar relativamente próximos entre sí. Sin embargo, cuando aumentan las dimensiones:\n",
    "\n",
    "* Los puntos tienden a estar muy separados.\n",
    "* Las distancias entre ellos se vuelven más similares.\n",
    "* Resulta más difícil distinguir grupos naturales.\n",
    "\n",
    "En otras palabras, “hay demasiado espacio”. Los datos se dispersan.\n",
    "\n",
    "Esto implica que:\n",
    "\n",
    "* Se necesitan muchos más datos para cubrir bien el espacio.\n",
    "* Los algoritmos de clustering pueden funcionar peor.\n",
    "* Las métricas de distancia pierden capacidad discriminatoria.\n",
    "\n",
    "---\n",
    "\n",
    "## Relación con el sobreajuste\n",
    "\n",
    "Cuantas más dimensiones tenga un conjunto de datos:\n",
    "\n",
    "* Más compleja puede volverse la frontera de decisión.\n",
    "* Más fácil es que el modelo aprenda patrones específicos del entrenamiento.\n",
    "* Mayor es el riesgo de sobreajuste.\n",
    "\n",
    "Un modelo puede parecer muy preciso en entrenamiento, pero no generalizar bien a datos nuevos.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "1AVz50uWpCv6"
   },
   "source": [
    "# 2. Principales aproximaciones a la reducción de dimensionalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Am_cSSVopCv-"
   },
   "source": [
    "---\n",
    "\n",
    "## 2.1 Proyección\n",
    "\n",
    "En muchos problemas reales, aunque trabajemos con muchas variables, los datos no se distribuyen de forma uniforme en todas las dimensiones. Es habitual que:\n",
    "\n",
    "* Algunas características apenas varíen.\n",
    "* Otras estén fuertemente correlacionadas.\n",
    "* Parte de la información sea redundante.\n",
    "\n",
    "Como consecuencia, las instancias de entrenamiento no ocupan todo el espacio de alta dimensión, sino que se concentran en un subespacio de menor dimensión dentro de ese espacio.\n",
    "\n",
    "En la siguiente imagen se muestra un ejemplo conceptual. Aunque los datos están definidos en un espacio tridimensional, la mayoría de los puntos se sitúan cerca de un plano inclinado:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:50%\" src=\"img/subspace_projection.png\" />\n",
    "</div>\n",
    "\n",
    "Esto significa que, aunque formalmente trabajamos en 3 dimensiones, la estructura real de los datos es prácticamente bidimensional.\n",
    "\n",
    "---\n",
    "\n",
    "### Proyección sobre el subespacio\n",
    "\n",
    "Si proyectamos perpendicularmente cada instancia sobre ese plano (el subespacio), obtenemos una representación con menos dimensiones que conserva la estructura principal de los datos.\n",
    "\n",
    "El resultado sería algo similar a la siguiente imagen, donde los datos se representan en 2 dimensiones:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:50%\" src=\"img/2d_projection.png\" />\n",
    "</div>\n",
    "\n",
    "Ahora el problema se ha simplificado:\n",
    "\n",
    "* Hemos pasado de 3 dimensiones a 2.\n",
    "* Se mantiene la forma general de la distribución.\n",
    "* Se reduce la complejidad del espacio.\n",
    "\n",
    "---\n",
    "\n",
    "### Limitación de la proyección directa\n",
    "\n",
    "Sin embargo, la proyección no siempre es tan sencilla.\n",
    "\n",
    "En muchos casos, el subespacio donde se concentran los datos no está alineado con los ejes originales (las variables tal como están definidas en el dataset). Puede estar girado o inclinado respecto a ellos.\n",
    "\n",
    "Si eliminamos variables directamente sin tener en cuenta esa orientación, podríamos perder información importante.\n",
    "\n",
    "Por ello, los métodos de reducción de dimensionalidad más avanzados, como PCA, no solo proyectan, sino que primero buscan la mejor orientación posible del subespacio antes de realizar la proyección.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "1vkaVX-LpCwB"
   },
   "source": [
    "## 2.2 Aprendizaje en variedades (Manifold Learning)\n",
    "\n",
    "En el apartado anterior vimos que, aunque los datos puedan estar definidos en un espacio de muchas dimensiones, en realidad suelen concentrarse en una “superficie” de menor dimensión dentro de ese espacio.\n",
    "\n",
    "A esa idea se le llama variedad (manifold).\n",
    "\n",
    "Una variedad 2D es una superficie de dos dimensiones que puede estar doblada o curvada dentro de un espacio de mayor dimensión. Por ejemplo, una hoja de papel es bidimensional, aunque pueda estar arrugada o doblada en el espacio tridimensional.\n",
    "\n",
    "De forma más general, muchos conjuntos de datos de alta dimensión no ocupan todo el espacio posible, sino que se concentran cerca de una estructura de menor dimensión.\n",
    "\n",
    "---\n",
    "\n",
    "### Hipótesis de la variedad\n",
    "\n",
    "Muchos algoritmos de reducción de dimensionalidad se basan en la llamada hipótesis de la variedad.\n",
    "\n",
    "Esta hipótesis afirma que:\n",
    "\n",
    "La mayoría de los datos reales de alta dimensión se encuentran cerca de una estructura de menor dimensión.\n",
    "\n",
    "Esta idea no es solo teórica, sino que se observa frecuentemente en la práctica.\n",
    "\n",
    "---\n",
    "\n",
    "### Ejemplo con imágenes 28x28\n",
    "\n",
    "Si generamos todas las imágenes posibles en una cuadrícula de 28x28 píxeles, el número total de combinaciones posibles es enorme.\n",
    "\n",
    "Sin embargo, solo una pequeña parte de esas combinaciones corresponde a imágenes que realmente se parecen a dígitos escritos a mano.\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:50%\" src=\"img/mnist-digit-7.png\" alt=\"mnist-digit-7\"  />\n",
    "</div>\n",
    "\n",
    "Esto significa que, aunque matemáticamente el espacio de todas las imágenes posibles es gigantesco, las imágenes reales de dígitos ocupan una región mucho más pequeña y estructurada dentro de ese espacio.\n",
    "\n",
    "Esa región puede interpretarse como una variedad de menor dimensión.\n",
    "\n",
    "Las restricciones físicas y estructurales del problema reducen enormemente los grados de libertad reales.\n",
    "\n",
    "---\n",
    "\n",
    "### Clasificación en un espacio curvado\n",
    "\n",
    "La siguiente imagen muestra un ejemplo donde los datos están organizados en una estructura curva dentro de un espacio de mayor dimensión:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:50%\" src=\"img/manifold_classification.png\" alt=\"manifold_classification\"/>\n",
    "</div>\n",
    "\n",
    "En algunos casos, si conseguimos “desenrollar” esa estructura y trabajar en la variedad subyacente, el problema de clasificación puede volverse más sencillo.\n",
    "\n",
    "Sin embargo, esto no siempre ocurre.\n",
    "\n",
    "Reducir la dimensionalidad puede acelerar el entrenamiento y simplificar la representación, pero no garantiza automáticamente una mejor solución. Todo depende del conjunto de datos y de la tarea concreta.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "kz6jIBRspCwG"
   },
   "source": [
    "# 3. PCA: Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "m5StopHSpCwJ"
   },
   "source": [
    "El análisis de componentes principales (PCA) es, con diferencia, el algoritmo de reducción de dimensionalidad más popular en aprendizaje automático.\n",
    "\n",
    "PCA transforma un conjunto de variables posiblemente correlacionadas en un nuevo conjunto de variables no correlacionadas llamadas componentes principales, ordenadas según la cantidad de información que conservan.\n",
    "\n",
    "PCA es un algoritmo sensible a la escala relativa de las variables originales. Si una variable tiene valores numéricamente más grandes que otra, influirá más en el cálculo de los componentes principales. Por ello, antes de aplicar PCA es habitual estandarizar los datos (media 0 y desviación típica 1).\n",
    "\n",
    "El PCA fue inventado en 1901 por [Karl Pearson](https://en.wikipedia.org/wiki/Karl_Pearson) como un análogo del teorema del eje principal en mecánica. Más tarde fue desarrollado de forma independiente y nombrado por [Harold Hotelling](https://en.wikipedia.org/wiki/Harold_Hotelling) en la década de 1930. Aunque su origen es antiguo, sigue siendo una herramienta fundamental en ciencia de datos actual.\n",
    "\n",
    "Para aplicar PCA, primero identificamos el hiperplano que mejor se ajusta a los datos y después proyectamos los datos sobre él.\n",
    "\n",
    "Un hiperplano puede entenderse como:\n",
    "\n",
    "* Una recta en 2 dimensiones.\n",
    "* Un plano en 3 dimensiones.\n",
    "* Una superficie equivalente en dimensiones superiores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "wTB1QkRdpCwO"
   },
   "source": [
    "## 3.1 Preservación de la varianza\n",
    "\n",
    "El objetivo de PCA es elegir el hiperplano que conserve la mayor varianza posible.\n",
    "\n",
    "La varianza mide cuánto se dispersan los datos. Si proyectamos los puntos sobre distintas direcciones, algunas conservarán más información que otras.\n",
    "\n",
    "En la siguiente imagen se muestran tres posibles ejes de proyección en un espacio 2D:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img style=\"width:66%\" src=\"img/2D_variance_projection.png\" />\n",
    "</div>\n",
    "\n",
    "Parece razonable seleccionar el eje que conserva la mayor cantidad de varianza, ya que es el que menos información pierde respecto a los datos originales.\n",
    "\n",
    "Otra forma equivalente de verlo es la siguiente: al elegir el eje que maximiza la varianza, estamos minimizando la distancia cuadrática media entre los puntos originales y sus proyecciones sobre ese eje. Esta es la idea central detrás de PCA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "8XbqRGQdpCwX"
   },
   "source": [
    "## 3.2 Componentes principales\n",
    "\n",
    "PCA identifica:\n",
    "\n",
    "* El eje que representa la mayor cantidad de variación en el conjunto de entrenamiento (primer componente principal).\n",
    "* Un segundo eje, ortogonal al primero, que captura la mayor cantidad de varianza restante (segundo componente principal).\n",
    "* Y así sucesivamente.\n",
    "\n",
    "Si el conjunto de datos tiene n dimensiones originales, existirán n componentes principales posibles, aunque normalmente solo se conservan las primeras.\n",
    "\n",
    "El i-ésimo eje se denomina i-ésimo componente principal.\n",
    "\n",
    "Cada componente principal es una combinación lineal de las variables originales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo se calculan los componentes principales?\n",
    "\n",
    "Existe una técnica estándar de álgebra lineal llamada Descomposición de Valores Singulares (SVD). Esta técnica descompone la matriz de datos $X$ como:\n",
    "\n",
    "$\n",
    "X = U \\Sigma V^T\n",
    "$\n",
    "\n",
    "La matriz $V$ contiene los vectores unitarios que definen las direcciones de los componentes principales:\n",
    "\n",
    "$$V= \\begin{pmatrix} \\vert & \\vert & \\dots & \\vert \\\\ c_1 & c_2 & \\dots & c_n \\\\ \\vert & \\vert & \\dots & \\vert \\\\ \\end{pmatrix} $$\n",
    "\n",
    "En la práctica, no es necesario implementar SVD manualmente. Librerías como NumPy o Scikit-learn realizan este cálculo de forma eficiente.\n",
    "\n",
    "En este curso el objetivo no es profundizar en la demostración matemática, sino:\n",
    "\n",
    "* Comprender qué representan los componentes principales.\n",
    "* Interpretar la varianza explicada.\n",
    "* Aplicar PCA correctamente en un entorno práctico.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "xwphpKYypCwd"
   },
   "source": [
    "## 3.3. PCA aplicado al dataset Wine\n",
    "\n",
    "### 3.3.1. Contexto: ¿Por qué aplicar PCA aquí?\n",
    "\n",
    "En teoría hemos visto que:\n",
    "\n",
    "* Muchos datasets tienen variables correlacionadas.\n",
    "* En espacios de alta dimensión los datos pueden dispersarse.\n",
    "* Reducir dimensionalidad puede simplificar el problema.\n",
    "* PCA busca conservar la máxima varianza posible.\n",
    "\n",
    "El dataset Wine contiene:\n",
    "\n",
    "* 13 variables químicas\n",
    "* 3 clases de vino\n",
    "* Variables que suelen estar correlacionadas\n",
    "\n",
    "Pregunta inicial:\n",
    "\n",
    "> ¿Podemos representar estos 13 atributos con menos dimensiones sin perder demasiada información?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. Cargar y preparar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1678,
     "status": "ok",
     "timestamp": 1681291755867,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -120
    },
    "id": "E2Blz99RpCwj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "wine = load_wine()\n",
    "\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3. Escalado de datos\n",
    "\n",
    "Recordatorio teórico: PCA es sensible a la escala.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HFRDqEpTpCxK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora todas las variables tienen:\n",
    "\n",
    "* Media 0\n",
    "* Desviación típica 1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4. Aplicación de PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "X_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_pca es una matriz en la que cada fila es un vino.\n",
    "\n",
    "Cada columna es:\n",
    "\n",
    "* X_pca[:, 0] → PC1\n",
    "* X_pca[:, 1] → PC2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.5. ¿Qué son PC1 y PC2?\n",
    "\n",
    "PCA crea nuevas variables llamadas Componentes Principales.\n",
    "\n",
    "##### PC1 (Primer Componente Principal)\n",
    "\n",
    "* Es la dirección donde los datos presentan mayor varianza.\n",
    "* Es una combinación lineal de las variables originales.\n",
    "* Contiene la mayor cantidad de información posible en una sola dimensión.\n",
    "\n",
    "##### PC2 (Segundo Componente Principal)\n",
    "\n",
    "* Es perpendicular a PC1.\n",
    "* Captura la mayor parte de la varianza restante.\n",
    "* También es combinación de las variables originales.\n",
    "\n",
    "En nuestro caso:\n",
    "\n",
    "* Hemos pasado de 13 dimensiones → 2 dimensiones.\n",
    "* PC1 y PC2 resumen la información original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "aYcQ6KmMpCxL",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.3.6. Varianza explicada\n",
    "\n",
    "PCA busca las direcciones que conservan la mayor cantidad posible de varianza. La varianza mide cuánto se dispersan los datos. Cuanta más varianza conserve un componente, más información del conjunto original está representando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1253,
     "status": "ok",
     "timestamp": 1681291792784,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -120
    },
    "id": "JFdY1566pCxM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "TJQmDGIdpCxO"
   },
   "source": [
    "Este atributo devuelve un array donde cada valor indica la proporción de la varianza total que explica cada componente principal.\n",
    "\n",
    "Esto significa:\n",
    "\n",
    "* El PC1 explica aproximadamente el 36 % de la varianza total.\n",
    "* El PC2 explica aproximadamente el 19 % de la varianza total.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto obtenemos la varianza acumulada, es decir, cuánta información total estamos conservando al mantener esos componentes.\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "<br>\n",
    "\n",
    "Reflexión:\n",
    "\n",
    "* ¿Cuánta información estamos conservando?\n",
    "* ¿Es suficiente?\n",
    "* ¿Qué ocurriría si usamos solo 1 componente?\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la práctica profesional, se suele elegir un número de componentes que conserve un alto porcentaje de la varianza total, por ejemplo:\n",
    "\n",
    "* 90 %\n",
    "* 95 %\n",
    "* 99 %\n",
    "\n",
    "Esto supone un equilibrio entre:\n",
    "\n",
    "* Reducir dimensionalidad.\n",
    "* No perder demasiada información."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "8vdSuNyGpCxO"
   },
   "source": [
    "### 3.3.7. Visualización\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1078,
     "status": "ok",
     "timestamp": 1681291930136,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -120
    },
    "id": "uiX5SfyUpCxP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "for label, color in zip([0,1,2], ['red','green','blue']):\n",
    "    subset = X_pca[y == label]\n",
    "    plt.scatter(subset[:,0], subset[:,1], label=f\"Clase {label}\")\n",
    "\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"Wine proyectado en los 2 primeros componentes\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "62fXbBY3pCxQ"
   },
   "source": [
    "### 3.3.8. Interpretación del gráfico\n",
    "\n",
    "Preguntas clave:\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "<br>\n",
    "\n",
    "* ¿Se observan grupos diferenciados?\n",
    "* ¿Hay separación clara entre clases?\n",
    "* ¿Qué indica esto sobre las variables originales?\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí se ve claramente que PCA:\n",
    "\n",
    "* Ha reducido dimensionalidad.\n",
    "* Mantiene estructura.\n",
    "* Permite visualización clara.\n",
    "\n",
    "Esto conecta directamente con la teoría de preservación de varianza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.9. Selección del número óptimo de componentes\n",
    "\n",
    "En PCA no fijamos el número de componentes de forma arbitraria.\n",
    "Una práctica habitual es analizar la varianza acumulada y elegir el número mínimo de componentes que conserve un alto porcentaje de la información (por ejemplo, 95 %).\n",
    "\n",
    "Primero entrenamos PCA utilizando todos los componentes posibles:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 459,
     "status": "ok",
     "timestamp": 1681291959413,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -120
    },
    "id": "uXOV8PRwpCxR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "varianza = pca_full.explained_variance_ratio_\n",
    "varianza_acumulada = np.cumsum(varianza)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.9.1. Método del codo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos una gráfica con la varianza individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(range(1, len(varianza)+1), varianza, marker='o')\n",
    "plt.xlabel(\"Número de componentes\")\n",
    "plt.ylabel(\"Varianza individual explicada\")\n",
    "plt.title(\"Varianza explicada por componente\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el gráfico no se ve claro, pero se puede inteuir que el codo está a partir del cuarto componente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.9.2. Componentes para un porcentaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a buscar un portentaje de 90%, para tomar una decisión objetiva, utilizamos la varianza acumulada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varianza_acumulada = np.cumsum(varianza)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(range(1, len(varianza_acumulada)+1),\n",
    "         varianza_acumulada,\n",
    "         marker='o')\n",
    "\n",
    "plt.axhline(y=0.90, color='r', linestyle='--')\n",
    "plt.xlabel(\"Número de componentes\")\n",
    "plt.ylabel(\"Varianza acumulada\")\n",
    "plt.title(\"Varianza acumulada - PCA (Wine)\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La línea roja marca el umbral del 90 %.\n",
    "\n",
    "Calculamos automáticamente el número mínimo de componentes necesarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.argmax(varianza_acumulada >= 0.90) + 1\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significa que necesitamos 8 componentes para conservar al menos el 90 % de la información original.\n",
    "\n",
    "Ahora aplicamos para reducir de 13 a 8.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_95 = PCA(n_components=0.90)\n",
    "X_reduced = pca_95.fit_transform(X_scaled)\n",
    "\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "SAnfOd7ZpCxT"
   },
   "source": [
    "### 3.3.10.  Comparación con modelo supervisado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZOKQS0fpCxT",
    "outputId": "0e78b5b9-6570-48ae-cc4d-600d4faaf414",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# -------------------------\n",
    "# SIN PCA\n",
    "# -------------------------\n",
    "start = time.time()\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "end = time.time()\n",
    "tiempo_original = end - start\n",
    "\n",
    "acc_original = accuracy_score(y_test, model.predict(X_test))\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# CON PCA\n",
    "# -------------------------\n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "model_pca = RandomForestClassifier()\n",
    "model_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "end = time.time()\n",
    "tiempo_pca = end - start\n",
    "\n",
    "acc_pca = accuracy_score(y_test, model_pca.predict(X_test_pca))\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Cálculo de diferencias porcentuales\n",
    "# -------------------------\n",
    "\n",
    "dif_accuracy = acc_pca - acc_original\n",
    "dif_tiempo = tiempo_original - tiempo_pca\n",
    "\n",
    "porcentaje_tiempo = (dif_tiempo / tiempo_original) * 100 if tiempo_original != 0 else 0\n",
    "porcentaje_accuracy = (dif_accuracy / acc_original) * 100 if acc_original != 0 else 0\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# RESULTADOS\n",
    "# -------------------------\n",
    "print(\"=== Comparación Random Forest ===\\n\")\n",
    "\n",
    "print(f\"Dimensiones originales: {X_train.shape[1]}\")\n",
    "print(f\"Dimensiones tras PCA: {X_train_pca.shape[1]}\\n\")\n",
    "\n",
    "print(f\"Accuracy sin PCA: {acc_original:.4f}\")\n",
    "print(f\"Accuracy con PCA: {acc_pca:.4f}\")\n",
    "print(f\"Diferencia de accuracy: {porcentaje_accuracy:.2f} %\\n\")\n",
    "\n",
    "print(f\"Tiempo sin PCA: {tiempo_original:.6f} segundos\")\n",
    "print(f\"Tiempo con PCA: {tiempo_pca:.6f} segundos\")\n",
    "print(f\"Reducción de tiempo: {porcentaje_tiempo:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "m3K5dVBopCxU"
   },
   "source": [
    "### 3.3.11.  Reflexión final\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "<br>\n",
    "\n",
    "Preguntas clave:\n",
    "\n",
    "* ¿Reduce dimensionalidad?\n",
    "* ¿Cuánta información conserva?\n",
    "* ¿Mejora o empeora la precisión?\n",
    "* ¿Cuándo usarías PCA en un proyecto real?\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "BfpGRS9LpCxX"
   },
   "source": [
    "## PCA para compresión de imágenes\n",
    "\n",
    "En una imagen digital, cada píxel es una variable.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "* Una imagen de 28 × 28 píxeles tiene\n",
    "  28 × 28 = **784 variables**.\n",
    "\n",
    "Esto significa que cada imagen puede interpretarse como un punto en un espacio de 784 dimensiones.\n",
    "\n",
    "En problemas reales de visión artificial:\n",
    "\n",
    "* Las imágenes pueden tener miles o millones de píxeles.\n",
    "* El entrenamiento de modelos puede volverse muy costoso.\n",
    "* Muchas zonas de la imagen contienen información redundante.\n",
    "\n",
    "PCA se utiliza en imágenes principalmente para:\n",
    "\n",
    "1. **Reducir dimensionalidad**\n",
    "\n",
    "   * Pasar de cientos o miles de píxeles a un número menor de componentes.\n",
    "   * Simplificar el problema.\n",
    "\n",
    "2. **Compresión**\n",
    "\n",
    "   * Guardar menos información manteniendo la estructura principal.\n",
    "   * Reducir espacio de almacenamiento.\n",
    "\n",
    "3. **Eliminar ruido**\n",
    "\n",
    "   * Las componentes principales conservan la información más importante.\n",
    "   * Las componentes pequeñas suelen representar ruido.\n",
    "\n",
    "4. **Acelerar entrenamiento**\n",
    "\n",
    "   * Menos dimensiones → menos cálculos.\n",
    "   * Modelos más rápidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo paso a paso con Olivetti Faces\n",
    "\n",
    "Ahora aplicaremos PCA al dataset Olivetti Faces, que contiene imágenes de rostros en escala de grises.\n",
    "\n",
    "Cada imagen:\n",
    "\n",
    "* Tamaño: 64 x 64\n",
    "* Dimensiones originales: 4096\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 1: Cargar Olivetti Faces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 972,
     "status": "ok",
     "timestamp": 1681293190290,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -120
    },
    "id": "Nc6CqjkJpCxY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "olivetti = fetch_olivetti_faces(shuffle=True, random_state=42)\n",
    "X, y = olivetti.data, olivetti.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Paso 2: Aplicar PCA conservando el 95 % de varianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 77613,
     "status": "ok",
     "timestamp": 1681293271660,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -120
    },
    "id": "xN_tInJypCxY",
    "outputId": "784973f1-2ec4-45cb-f033-4a3e89469b6a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "\n",
    "X_train_reduced = pca.fit_transform(X_train)\n",
    "X_test_reduced = pca.transform(X_test)\n",
    "\n",
    "print(\"Dimensiones originales:\", X_train.shape[1])\n",
    "print(\"Dimensiones tras PCA:\", X_train_reduced.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí veremos que:\n",
    "\n",
    "* Se pasa de 4096 dimensiones a un número mucho menor.\n",
    "* Se conserva el 95 % de la información.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Paso 3: Reconstruir imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 971,
     "status": "ok",
     "timestamp": 1681293282830,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -120
    },
    "id": "o70ltWwcpCxY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_test_recovered = pca.inverse_transform(X_test_reduced)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Paso 4: Visualizar comparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 495,
     "status": "ok",
     "timestamp": 1681293288556,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -120
    },
    "id": "Q3IgOMhUpCxZ",
    "outputId": "46d6450c-9ffa-4c22-e950-a57185a27084",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(X_test[idx].reshape(64,64), cmap='gray')\n",
    "plt.title(\"Original\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(X_test_recovered[idx].reshape(64,64), cmap='gray')\n",
    "plt.title(\"Reconstruida (PCA 95%)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretación\n",
    "\n",
    "* La imagen reconstruida se parece mucho a la original.\n",
    "* Puede verse ligeramente más borrosa.\n",
    "* Esa diferencia es la información perdida al reducir dimensionalidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Error de reconstrucción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 543,
     "status": "ok",
     "timestamp": 1681293343251,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -120
    },
    "id": "ah3xWuEfpCxZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "error = np.mean((X_test - X_test_recovered) ** 2)\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuanto menor sea el error:\n",
    "\n",
    "* Mejor hemos conservado la información.\n",
    "* Más fiel es la reconstrucción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14371,
     "status": "ok",
     "timestamp": 1681293361627,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -120
    },
    "id": "gSsMzhoJpCxZ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "nU-4wOicpCxd"
   },
   "source": [
    "## PCA incremental\n",
    "\n",
    "Un problema del PCA clásico es que necesita cargar todo el conjunto de datos en memoria para calcular los componentes principales.\n",
    "\n",
    "Cuando el dataset es pequeño (como Wine o Titanic), esto no supone ningún inconveniente. Sin embargo, en problemas reales de Big Data, donde pueden existir millones de muestras, el conjunto completo puede no caber en memoria.\n",
    "\n",
    "Para resolver este problema existen algoritmos de **PCA incremental**.\n",
    "\n",
    "La idea es sencilla:\n",
    "\n",
    "* En lugar de procesar todo el dataset a la vez,\n",
    "* Se divide en pequeños bloques (mini-lotes),\n",
    "* Y se procesan uno a uno.\n",
    "\n",
    "De esta forma, el modelo va actualizando los componentes principales progresivamente sin necesidad de cargar todos los datos simultáneamente.\n",
    "\n",
    "---\n",
    "\n",
    "### ¿Cuándo es útil?\n",
    "\n",
    "El PCA incremental es útil cuando:\n",
    "\n",
    "* El dataset es muy grande.\n",
    "* La memoria disponible es limitada.\n",
    "* Los datos llegan en flujo continuo (streaming).\n",
    "\n",
    "Es importante destacar que:\n",
    "\n",
    "> PCA incremental no es necesariamente más rápido que el PCA clásico.\n",
    "> Su principal ventaja es el menor consumo de memoria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  PCA incremental aplicado a Olivetti Faces\n",
    "\n",
    "En el ejemplo anterior utilizamos PCA clásico para reducir las 4096 dimensiones de Olivetti Faces.\n",
    "\n",
    "Sin embargo, si el dataset fuera mucho mayor (millones de imágenes), podría no caber completamente en memoria. En ese caso, usaríamos PCA incremental, que procesa los datos por bloques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 1: Importar IncrementalPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 2: Definir el modelo\n",
    "\n",
    "Reduciremos dimensionalidad conservando, por ejemplo, 100 componentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipca = IncrementalPCA(n_components=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí estamos indicando:\n",
    "\n",
    "- No queremos conservar un porcentaje,\n",
    "\n",
    "- Queremos exactamente 100 componentes principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "\n",
    "for i in range(0, X_train.shape[0], batch_size):\n",
    "    X_batch = X_train[i:i+batch_size]\n",
    "    ipca.partial_fit(X_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí:\n",
    "\n",
    "* No cargamos todo el dataset en el algoritmo de una sola vez.\n",
    "\n",
    "* Se actualizan los componentes progresivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 4: Transformar los datos\n",
    "\n",
    "Una vez entrenado el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reduced = ipca.transform(X_train)\n",
    "X_test_reduced = ipca.transform(X_test)\n",
    "\n",
    "X_train_reduced.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diferencia clave con PCA clásico\n",
    "\n",
    "| PCA clásico                  | PCA incremental             |\n",
    "| ---------------------------- | --------------------------- |\n",
    "| Usa todo el dataset a la vez | Procesa por bloques         |\n",
    "| Más simple                   | Más escalable               |\n",
    "| Ideal para datasets pequeños | Ideal para datasets grandes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "JDSKHQMIpCxg"
   },
   "source": [
    "# Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "jxGtSd6jpCxg"
   },
   "source": [
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1. Fundamentos conceptuales\n",
    "\n",
    "1. Explica al menos **tres motivaciones** para reducir la dimensionalidad de un dataset.\n",
    "2. ¿Qué es la “maldición de la dimensionalidad”?\n",
    "3. ¿Cuáles son los principales inconvenientes de reducir dimensionalidad?\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "in-c5f3CpCxh"
   },
   "source": [
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "## 2. PCA aplicado a Olivetti Faces\n",
    "\n",
    "### 2.1 Preparación\n",
    "\n",
    "* Carga el dataset Olivetti Faces.\n",
    "* Divide en conjunto de entrenamiento y prueba.\n",
    "* Indica cuántas dimensiones tiene cada imagen.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "4oTED7KDpCxi"
   },
   "source": [
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "### 2.2 Clasificador sin PCA\n",
    "\n",
    "1. Entrena un clasificador Random Forest.\n",
    "2. Mide el tiempo de entrenamiento.\n",
    "3. Calcula la accuracy en test.\n",
    "4. Anota los resultados.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ju-hWEtbpCxj"
   },
   "source": [
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "### 2.3 Reducción con PCA\n",
    "\n",
    "1. Aplica PCA conservando el 95 % de la varianza.\n",
    "2. Indica cuántas dimensiones se han conservado.\n",
    "3. Explica qué significa conservar el 95 %.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "### 2.4 Clasificador con PCA\n",
    "\n",
    "1. Entrena el mismo clasificador con los datos reducidos.\n",
    "2. Mide el tiempo de entrenamiento.\n",
    "3. Calcula la accuracy en test.\n",
    "4. Compara con el modelo sin PCA.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "<br>\n",
    "\n",
    "Responde razonadamente:\n",
    "\n",
    "* ¿Ha mejorado el tiempo de entrenamiento?\n",
    "* ¿Ha mejorado la precisión?\n",
    "* ¿Compensa aplicar PCA en este caso?\n",
    "* ¿En qué tipo de problema sí sería especialmente útil?\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "## 3. PCA aplicado al dataset Breast Cancer\n",
    "\n",
    "Utilizaremos el dataset *load_breast_cancer* de scikit-learn.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1 Exploración inicial\n",
    "\n",
    "1. Carga el dataset.\n",
    "2. ¿Cuántas muestras tiene?\n",
    "3. ¿Cuántas variables?\n",
    "4. ¿Hay valores nulos?\n",
    "5. Analiza la matriz de correlación.\n",
    "6. ¿Crees que PCA puede ser útil aquí? Justifica.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "### 3.2 Análisis de varianza\n",
    "\n",
    "1. Escala los datos.\n",
    "2. Aplica PCA completo.\n",
    "3. Representa la varianza explicada individual.\n",
    "4. Representa la varianza acumulada.\n",
    "5. ¿Se observa un “codo”?\n",
    "6. ¿Cuántos componentes son necesarios para conservar el 95 %?\n",
    "7. Justifica tu elección.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "### 3.3 Visualización\n",
    "\n",
    "1. Reduce a 2 componentes.\n",
    "2. Representa PC1 vs PC2.\n",
    "3. Colorea por clase (maligno / benigno).\n",
    "4. ¿Se observan grupos diferenciados?\n",
    "5. ¿PCA ayuda a visualizar mejor la estructura del dataset?\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "### 3.4 Impacto en clasificación\n",
    "\n",
    "1. Divide en train/test.\n",
    "2. Entrena un clasificador (Random Forest o Logistic Regression) con las 30 variables originales.\n",
    "3. Calcula accuracy y tiempo de entrenamiento.\n",
    "4. Reduce dimensionalidad conservando el 95 %.\n",
    "5. Entrena el mismo modelo con los datos reducidos.\n",
    "6. Compara resultados.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "<br>\n",
    "\n",
    "Preguntas obligatorias:\n",
    "\n",
    "* ¿Mejora la precisión?\n",
    "* ¿Se reduce el tiempo?\n",
    "* ¿Compensa aplicar PCA?\n",
    "* ¿Qué ocurre si reduces demasiado el número de componentes?\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "<br>\n",
    "\n",
    "1. ¿En qué tipo de dataset es especialmente útil PCA?\n",
    "2. ¿En qué casos puede empeorar el rendimiento?\n",
    "3. Diferencia entre:\n",
    "\n",
    "   * PCA clásico\n",
    "   * PCA aleatorio\n",
    "   * PCA incremental\n",
    "4. ¿PCA es un algoritmo supervisado o no supervisado? Justifica.\n",
    "\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}